from mage_ai.data_preparation.decorators import streaming_sink
from feast import FeatureStore
import pandas as pd
from datetime import datetime
import logging
import os

# Configure logging
logging.basicConfig(level=logging.INFO)

@streaming_sink
def export_data(*, data, **kwargs):
    """
    Exports transformed data to Feast by writing to a Parquet file.

    Args:
        data (List[Dict]): List of transformed messages.
    """
    logging.info(f"Exporting {len(data)} records to Feast.")
    
    # Define absolute path to ensure correct file location
    base_dir = os.path.dirname(os.path.abspath(__file__))
    parquet_path = os.path.join(base_dir, '../data/client_features.parquet')
    
    # Initialize the Feast Feature Store
    store = FeatureStore(repo_path=os.path.join(base_dir, '../feature_repo'))
    
    # Prepare data for Feast
    feature_rows = []
    for record in data:
        features = record.get('features', {})
        feature_rows.append({
            'client_id': record.get('client_id'),
            'age': features.get('age'),
            'income': features.get('income'),
            'gender': features.get('gender'),
            'event_timestamp': datetime.utcnow(),
        })
    
    feature_df = pd.DataFrame(feature_rows)
    
    # Ensure the data directory exists
    data_dir = os.path.dirname(parquet_path)
    os.makedirs(data_dir, exist_ok=True)
    
    # Write features to Feast offline store
    try:
        if os.path.exists(parquet_path):
            existing_df = pd.read_parquet(parquet_path)
            feature_df = pd.concat([existing_df, feature_df], ignore_index=True)
        
        feature_df.to_parquet(parquet_path, index=False)
        logging.info(f"Successfully wrote to {parquet_path}")
    except Exception as e:
        logging.error(f"Failed to write to {parquet_path}: {e}")
        raise